# ğ—Ÿğ—²ğ˜â€™ğ˜€ ğ˜€ğ˜ğ—®ğ—¿ğ˜ ğ˜„ğ—¶ğ˜ğ—µ ğ—¨ğ—»ğ˜€ğ˜‚ğ—½ğ—²ğ—¿ğ˜ƒğ—¶ğ˜€ğ—²ğ—± ğ—Ÿğ—²ğ—®ğ—¿ğ—»ğ—¶ğ—»ğ—´ ğ—®ğ—»ğ—± ğ˜ğ—µğ—²ğ—» ğ—ºğ—¼ğ˜ƒğ—² ğ—¼ğ—» ğ˜ğ—¼ ğ—¶ğ—»-ğ—±ğ—²ğ—½ğ˜ğ—µ ğ——ğ—²ğ—²ğ—½ ğ—Ÿğ—²ğ—®ğ—¿ğ—»ğ—¶ğ—»ğ—´ ğ—°ğ—¼ğ—»ğ—°ğ—²ğ—½ğ˜ğ˜€

---
- [ğ—¨ğ—»ğ˜€ğ˜‚ğ—½ğ—²ğ—¿ğ˜ƒğ—¶ğ˜€ğ—²ğ—± ğ—Ÿğ—²ğ—®ğ—¿ğ—»ğ—¶ğ—»ğ—´](https://github.com/Ratnesh-181998/Unsupervised-and-Reinforcement-Learning/blob/main/Unsupervised%20Learning.pdf)
- Unsupervised learning is a class of machine learning techniques that aims to discover hidden patterns, structure, or representations in data ğ˜„ğ—¶ğ˜ğ—µğ—¼ğ˜‚ğ˜ ğ˜ğ—µğ—² ğ˜‚ğ˜€ğ—² ğ—¼ğ—³ ğ—¹ğ—®ğ—¯ğ—²ğ—¹ğ—²ğ—± ğ—¼ğ˜‚ğ˜ğ—½ğ˜‚ğ˜ğ˜€. Unlike supervised learning, where models learn a mapping from inputs to known targets, unsupervised learning operates only on the ğ—¶ğ—»ğ—½ğ˜‚ğ˜ ğ˜€ğ—½ğ—®ğ—°ğ—² ğ—®ğ—»ğ—± ğ—¿ğ—²ğ—¹ğ—¶ğ—²ğ˜€ ğ—¼ğ—» ğ—¶ğ—»ğ˜ğ—¿ğ—¶ğ—»ğ˜€ğ—¶ğ—° ğ—½ğ—¿ğ—¼ğ—½ğ—²ğ—¿ğ˜ğ—¶ğ—²ğ˜€ ğ—¼ğ—³ ğ˜ğ—µğ—² ğ—±ğ—®ğ˜ğ—®. It is essential for exploratory data analysis, feature learning, dimensionality reduction, clustering, and representation learning in modern AI systems.
Let the dataset be ğ—« = {ğ˜…â‚, ğ˜…â‚‚, â€¦, ğ˜…â‚™}, where each xáµ¢ âˆˆ Ráµˆ. The objective of unsupervised learning is to model the underlying distribution p(x) or to identify meaningful structure within X. One major category is ğ—°ğ—¹ğ˜‚ğ˜€ğ˜ğ—²ğ—¿ğ—¶ğ—»ğ—´, where the goal is to ğ—½ğ—®ğ—¿ğ˜ğ—¶ğ˜ğ—¶ğ—¼ğ—» ğ—±ğ—®ğ˜ğ—® ğ—¶ğ—»ğ˜ğ—¼ ğ—´ğ—¿ğ—¼ğ˜‚ğ—½ğ˜€ ğ˜ğ—µğ—®ğ˜ ğ—ºğ—®ğ˜…ğ—¶ğ—ºğ—¶ğ˜‡ğ—² ğ˜„ğ—¶ğ˜ğ—µğ—¶ğ—»-ğ—´ğ—¿ğ—¼ğ˜‚ğ—½ ğ˜€ğ—¶ğ—ºğ—¶ğ—¹ğ—®ğ—¿ğ—¶ğ˜ğ˜† ğ—®ğ—»ğ—± ğ—ºğ—¶ğ—»ğ—¶ğ—ºğ—¶ğ˜‡ğ—² ğ—¯ğ—²ğ˜ğ˜„ğ—²ğ—²ğ—»-ğ—´ğ—¿ğ—¼ğ˜‚ğ—½ ğ˜€ğ—¶ğ—ºğ—¶ğ—¹ğ—®ğ—¿ğ—¶ğ˜ğ˜†. In K-means clustering, this is formulated as minimizing the objective: ğ— = Î£â‚– Î£_{ğ˜…áµ¢âˆˆğ—–â‚–} ||ğ˜…áµ¢ âˆ’ Î¼â‚–||Â²

- where Î¼â‚– is the ğ—°ğ—²ğ—»ğ˜ğ—¿ğ—¼ğ—¶ğ—± ğ—¼ğ—³ ğ—°ğ—¹ğ˜‚ğ˜€ğ˜ğ—²ğ—¿ ğ—–â‚–. This objective encourages compact and well-separated clusters.
- Another important class of unsupervised methods is ğ—±ğ—¶ğ—ºğ—²ğ—»ğ˜€ğ—¶ğ—¼ğ—»ğ—®ğ—¹ğ—¶ğ˜ğ˜† ğ—¿ğ—²ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—», which seeks a low-dimensional representation z âˆˆ Ráµ, k < d, that preserves important information. - In ğ—£ğ—¿ğ—¶ğ—»ğ—°ğ—¶ğ—½ğ—®ğ—¹ ğ—–ğ—¼ğ—ºğ—½ğ—¼ğ—»ğ—²ğ—»ğ˜ ğ—”ğ—»ğ—®ğ—¹ğ˜†ğ˜€ğ—¶ğ˜€, the transformation is linear and defined as z = Wáµ€x, where W consists of eigenvectors of the covariance matrix Î£ = (1/ğ—»)ğ—«áµ€ğ—«. 
- The objective is to ğ—ºğ—®ğ˜…ğ—¶ğ—ºğ—¶ğ˜‡ğ—² ğ˜ƒğ—®ğ—¿ğ—¶ğ—®ğ—»ğ—°ğ—²: ğ—ºğ—®ğ˜… ğ—©ğ—®ğ—¿(ğ—ªáµ€ğ—«).
- Density estimation is another core unsupervised task, where the goal is to approximate p(x). ğ—šğ—®ğ˜‚ğ˜€ğ˜€ğ—¶ğ—®ğ—» ğ— ğ—¶ğ˜…ğ˜ğ˜‚ğ—¿ğ—² ğ— ğ—¼ğ—±ğ—²ğ—¹ğ˜€ represent the distribution as a weighted sum of Gaussians:ğ—½(ğ˜…) = Î£ Ï€â‚– ğ—¡(ğ˜… | Î¼â‚–, Î£â‚–)
- Parameters are learned using the ğ—˜ğ˜…ğ—½ğ—²ğ—°ğ˜ğ—®ğ˜ğ—¶ğ—¼ğ—»â€“ğ— ğ—®ğ˜…ğ—¶ğ—ºğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—®ğ—¹ğ—´ğ—¼ğ—¿ğ—¶ğ˜ğ—µğ—º, which alternates between computing responsibilities and maximizing likelihood.
- In modern generative models such as ğ—®ğ˜‚ğ˜ğ—¼ğ—²ğ—»ğ—°ğ—¼ğ—±ğ—²ğ—¿ğ˜€, unsupervised learning minimizes reconstruction loss: ğ—Ÿ = ||ğ˜… âˆ’ ğ—³(ğ—´(ğ˜…))||Â²
where g is an encoder and f is a decoder. Unsupervised learning thus enables machines to discover structure, compress information, and learn representations, forming the foundation for clustering, anomaly detection, and generative AI systems.

---

# The role of Reinforcement Learning (RL)

- [Reinforcement Learning ](https://github.com/Ratnesh-181998/AI-Engineer/blob/main/Reinforcement%20Learning%20(RL)%20is%20a%20type%20of%20machine%20Learning.pdf)
  
<img width="346" height="326" alt="image" src="https://github.com/user-attachments/assets/8acbd862-2c83-408e-b59b-0d01755974aa" />
<img width="618" height="845" alt="image" src="https://github.com/user-attachments/assets/5daa222e-ddcb-41a5-8da9-551efe9c0f01" />
<img width="505" height="563" alt="image" src="https://github.com/user-attachments/assets/bb2f1505-ab2b-4ed2-be39-f4a2a602c709" />
<img width="493" height="430" alt="image" src="https://github.com/user-attachments/assets/dd1ab5e8-b849-46b1-bff7-7099ce5a7f8a" />
<img width="493" height="506" alt="image" src="https://github.com/user-attachments/assets/44fc97cf-1da2-42d6-9d94-94e5434cfe40" />
<img width="488" height="682" alt="image" src="https://github.com/user-attachments/assets/184b4c1c-0b4f-45a5-8947-2aa459c9cb69" />

---

# ğ—”ğ—» ğ—œğ—»-ğ——ğ—²ğ—½ğ˜ğ—µ ğ—¦ğ˜ğ˜‚ğ—±ğ˜† ğ—¼ğ—³ ğ—šğ—¿ğ—®ğ—±ğ—¶ğ—²ğ—»ğ˜ ğ—•ğ—¼ğ—¼ğ˜€ğ˜ğ—¶ğ—»ğ—´ 

- [ğ—šğ—¿ğ—®ğ—±ğ—¶ğ—²ğ—»ğ˜ ğ—•ğ—¼ğ—¼ğ˜€ğ˜ğ—¶ğ—»ğ—´]()
- Gradient Boosting is a powerful ensemble learning technique used for both ğ—°ğ—¹ğ—®ğ˜€ğ˜€ğ—¶ğ—³ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—®ğ—»ğ—± ğ—¿ğ—²ğ—´ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ—¼ğ—» ğ˜ğ—®ğ˜€ğ—¸ğ˜€. It builds models sequentially, ğ˜„ğ—µğ—²ğ—¿ğ—² ğ—²ğ—®ğ—°ğ—µ ğ—»ğ—²ğ˜„ ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—®ğ˜ğ˜ğ—²ğ—ºğ—½ğ˜ğ˜€ ğ˜ğ—¼ ğ—°ğ—¼ğ—¿ğ—¿ğ—²ğ—°ğ˜ ğ˜ğ—µğ—² ğ—²ğ—¿ğ—¿ğ—¼ğ—¿ğ˜€ ğ—ºğ—®ğ—±ğ—² ğ—¯ğ˜† ğ˜ğ—µğ—² ğ—°ğ—¼ğ—ºğ—¯ğ—¶ğ—»ğ—²ğ—± ğ—²ğ—»ğ˜€ğ—²ğ—ºğ—¯ğ—¹ğ—² ğ—¼ğ—³ ğ—½ğ—¿ğ—²ğ˜ƒğ—¶ğ—¼ğ˜‚ğ˜€ models. Unlike AdaBoost, which adjusts sample weights explicitly, ğ—šğ—¿ğ—®ğ—±ğ—¶ğ—²ğ—»ğ˜ ğ—•ğ—¼ğ—¼ğ˜€ğ˜ğ—¶ğ—»ğ—´ ğ—¼ğ—½ğ˜ğ—¶ğ—ºğ—¶ğ˜‡ğ—²ğ˜€ ğ—® ğ˜€ğ—½ğ—²ğ—°ğ—¶ğ—³ğ—¶ğ—²ğ—± ğ—¹ğ—¼ğ˜€ğ˜€ ğ—³ğ˜‚ğ—»ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ˜‚ğ˜€ğ—¶ğ—»ğ—´ ğ—´ğ—¿ğ—®ğ—±ğ—¶ğ—²ğ—»ğ˜ ğ—±ğ—²ğ˜€ğ—°ğ—²ğ—»ğ˜ ğ—½ğ—¿ğ—¶ğ—»ğ—°ğ—¶ğ—½ğ—¹ğ—²ğ˜€.

- The core idea of Gradient Boosting is to construct an additive model of weak learners, usually shallow decision trees. Let the model prediction after t iterations be denoted as Å·â‚œ(x). The model is updated iteratively as: Å·â‚œ(ğ˜…) = Å·â‚œâ‚‹â‚(ğ˜…) + Î· ğ—µâ‚œ(ğ˜…)

- where hâ‚œ(x) is the new weak learner added at iteration t, and Î· is the learning rate that controls the contribution of each learner.

- At each iteration, Gradient Boosting fits a new model to the negative gradient of the loss function with respect to the current predictions. For a given loss function L(y, Å·), the residuals are computed as: ğ—¿áµ¢â‚œ = âˆ’ âˆ‚ğ—Ÿ(ğ˜†áµ¢, Å·áµ¢) / âˆ‚Å·áµ¢

- ğ—§ğ—µğ—²ğ˜€ğ—² ğ—¿ğ—²ğ˜€ğ—¶ğ—±ğ˜‚ğ—®ğ—¹ğ˜€ ğ—¿ğ—²ğ—½ğ—¿ğ—²ğ˜€ğ—²ğ—»ğ˜ ğ˜ğ—µğ—² ğ—±ğ—¶ğ—¿ğ—²ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—¶ğ—» ğ˜„ğ—µğ—¶ğ—°ğ—µ ğ˜ğ—µğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—»ğ—²ğ—²ğ—±ğ˜€ ğ˜ğ—¼ ğ—®ğ—±ğ—·ğ˜‚ğ˜€ğ˜ ğ—¶ğ˜ğ˜€ ğ—½ğ—¿ğ—²ğ—±ğ—¶ğ—°ğ˜ğ—¶ğ—¼ğ—»ğ˜€ ğ˜ğ—¼ ğ—¿ğ—²ğ—±ğ˜‚ğ—°ğ—² ğ—²ğ—¿ğ—¿ğ—¼ğ—¿. The weak learner hâ‚œ(x) is trained to predict these residuals rather than the original target values.
- For example, in regression with squared error loss: ğ—Ÿ(ğ˜†, Å·) = Â½ (ğ˜† âˆ’ Å·)Â²

- the negative gradient simplifies to: ğ—¿áµ¢â‚œ = ğ˜†áµ¢ âˆ’ Å·áµ¢

- which are simply the ğ—¿ğ—²ğ˜€ğ—¶ğ—±ğ˜‚ğ—®ğ—¹ ğ—²ğ—¿ğ—¿ğ—¼ğ—¿ğ˜€.

- Once the weak learner is trained, its predictions are scaled by the ğ—¹ğ—²ğ—®ğ—¿ğ—»ğ—¶ğ—»ğ—´ ğ—¿ğ—®ğ˜ğ—² and added to the existing model. This process is repeated for a fixed number of iterations or until convergence.

- Gradient Boosting offers high flexibility, as it allows the choice of different loss functions, such as ğ—¹ğ—¼ğ—´ğ—¶ğ˜€ğ˜ğ—¶ğ—° ğ—¹ğ—¼ğ˜€ğ˜€ ğ—³ğ—¼ğ—¿ ğ—°ğ—¹ğ—®ğ˜€ğ˜€ğ—¶ğ—³ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—®ğ—»ğ—± ğ—›ğ˜‚ğ—¯ğ—²ğ—¿ ğ—¹ğ—¼ğ˜€ğ˜€ ğ—³ğ—¼ğ—¿ ğ—¿ğ—¼ğ—¯ğ˜‚ğ˜€ğ˜ğ—»ğ—²ğ˜€ğ˜€. However, it is sensitive to overfitting and requires careful tuning of hyperparameters like learning rate, tree depth, and number of estimators.

- Despite its complexity, Gradient Boosting remains one of the most effective algorithms for structured data problems and forms the foundation of advanced methods such as XGBoost and LightGBM.

---

# Biasâ€“Variance Tradeoff 

<img width="412" height="723" alt="image" src="https://github.com/user-attachments/assets/9a01738f-8d41-4e42-aed3-eb8933e3f8eb" />

<img width="875" height="483" alt="image" src="https://github.com/user-attachments/assets/1a17665f-3ca7-4ea8-9f0a-7233900de0f0" />

---
